{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GPR to predict the number of atoms\n",
    "## input spectra, for predict Cu,Te seperately and predict them both \n",
    "## input 1st and/or 2nd derivative data to predict Cu,Te together\n",
    "### summary: for individual predict, spectra better predict Te, derivate better predict Cu; If predict Cu and Te together, using first derivative data will give the best result. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CN</th>\n",
       "      <th>Num Cu</th>\n",
       "      <th>Num Te</th>\n",
       "      <th>Mu1</th>\n",
       "      <th>Mu2</th>\n",
       "      <th>Mu3</th>\n",
       "      <th>Mu4</th>\n",
       "      <th>Mu5</th>\n",
       "      <th>Mu6</th>\n",
       "      <th>Mu7</th>\n",
       "      <th>...</th>\n",
       "      <th>Mu91</th>\n",
       "      <th>Mu92</th>\n",
       "      <th>Mu93</th>\n",
       "      <th>Mu94</th>\n",
       "      <th>Mu95</th>\n",
       "      <th>Mu96</th>\n",
       "      <th>Mu97</th>\n",
       "      <th>Mu98</th>\n",
       "      <th>Mu99</th>\n",
       "      <th>Mu100</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9.920706</td>\n",
       "      <td>5.641016</td>\n",
       "      <td>4.279690</td>\n",
       "      <td>0.030303</td>\n",
       "      <td>0.032757</td>\n",
       "      <td>0.035112</td>\n",
       "      <td>0.037475</td>\n",
       "      <td>0.039958</td>\n",
       "      <td>0.042668</td>\n",
       "      <td>0.045714</td>\n",
       "      <td>...</td>\n",
       "      <td>0.994119</td>\n",
       "      <td>0.995957</td>\n",
       "      <td>0.998090</td>\n",
       "      <td>1.000540</td>\n",
       "      <td>1.003327</td>\n",
       "      <td>1.006472</td>\n",
       "      <td>1.009996</td>\n",
       "      <td>1.013918</td>\n",
       "      <td>1.018260</td>\n",
       "      <td>1.023043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9.038954</td>\n",
       "      <td>4.685766</td>\n",
       "      <td>4.353189</td>\n",
       "      <td>0.031529</td>\n",
       "      <td>0.034154</td>\n",
       "      <td>0.036671</td>\n",
       "      <td>0.039203</td>\n",
       "      <td>0.041875</td>\n",
       "      <td>0.044810</td>\n",
       "      <td>0.048132</td>\n",
       "      <td>...</td>\n",
       "      <td>0.999269</td>\n",
       "      <td>1.000942</td>\n",
       "      <td>1.002782</td>\n",
       "      <td>1.004773</td>\n",
       "      <td>1.006900</td>\n",
       "      <td>1.009148</td>\n",
       "      <td>1.011502</td>\n",
       "      <td>1.013947</td>\n",
       "      <td>1.016467</td>\n",
       "      <td>1.019047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>9.725820</td>\n",
       "      <td>5.599930</td>\n",
       "      <td>4.125890</td>\n",
       "      <td>0.030576</td>\n",
       "      <td>0.033095</td>\n",
       "      <td>0.035505</td>\n",
       "      <td>0.037921</td>\n",
       "      <td>0.040456</td>\n",
       "      <td>0.043223</td>\n",
       "      <td>0.046335</td>\n",
       "      <td>...</td>\n",
       "      <td>0.994823</td>\n",
       "      <td>0.997591</td>\n",
       "      <td>1.000637</td>\n",
       "      <td>1.003974</td>\n",
       "      <td>1.007616</td>\n",
       "      <td>1.011577</td>\n",
       "      <td>1.015872</td>\n",
       "      <td>1.020514</td>\n",
       "      <td>1.025518</td>\n",
       "      <td>1.030897</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9.374672</td>\n",
       "      <td>5.139779</td>\n",
       "      <td>4.234893</td>\n",
       "      <td>0.030891</td>\n",
       "      <td>0.033427</td>\n",
       "      <td>0.035861</td>\n",
       "      <td>0.038308</td>\n",
       "      <td>0.040883</td>\n",
       "      <td>0.043703</td>\n",
       "      <td>0.046883</td>\n",
       "      <td>...</td>\n",
       "      <td>0.999852</td>\n",
       "      <td>1.002114</td>\n",
       "      <td>1.004576</td>\n",
       "      <td>1.007246</td>\n",
       "      <td>1.010132</td>\n",
       "      <td>1.013241</td>\n",
       "      <td>1.016581</td>\n",
       "      <td>1.020158</td>\n",
       "      <td>1.023981</td>\n",
       "      <td>1.028057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9.599938</td>\n",
       "      <td>5.271755</td>\n",
       "      <td>4.328182</td>\n",
       "      <td>0.031077</td>\n",
       "      <td>0.033681</td>\n",
       "      <td>0.036170</td>\n",
       "      <td>0.038665</td>\n",
       "      <td>0.041287</td>\n",
       "      <td>0.044156</td>\n",
       "      <td>0.047393</td>\n",
       "      <td>...</td>\n",
       "      <td>0.998475</td>\n",
       "      <td>1.000035</td>\n",
       "      <td>1.001807</td>\n",
       "      <td>1.003776</td>\n",
       "      <td>1.005926</td>\n",
       "      <td>1.008242</td>\n",
       "      <td>1.010709</td>\n",
       "      <td>1.013311</td>\n",
       "      <td>1.016034</td>\n",
       "      <td>1.018861</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9995</th>\n",
       "      <td>9.360022</td>\n",
       "      <td>5.084340</td>\n",
       "      <td>4.275681</td>\n",
       "      <td>0.030950</td>\n",
       "      <td>0.033483</td>\n",
       "      <td>0.035915</td>\n",
       "      <td>0.038363</td>\n",
       "      <td>0.040943</td>\n",
       "      <td>0.043770</td>\n",
       "      <td>0.046960</td>\n",
       "      <td>...</td>\n",
       "      <td>1.001789</td>\n",
       "      <td>1.003715</td>\n",
       "      <td>1.005820</td>\n",
       "      <td>1.008111</td>\n",
       "      <td>1.010590</td>\n",
       "      <td>1.013262</td>\n",
       "      <td>1.016131</td>\n",
       "      <td>1.019201</td>\n",
       "      <td>1.022478</td>\n",
       "      <td>1.025964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9996</th>\n",
       "      <td>9.758551</td>\n",
       "      <td>5.435938</td>\n",
       "      <td>4.322612</td>\n",
       "      <td>0.030927</td>\n",
       "      <td>0.033519</td>\n",
       "      <td>0.035994</td>\n",
       "      <td>0.038473</td>\n",
       "      <td>0.041074</td>\n",
       "      <td>0.043918</td>\n",
       "      <td>0.047124</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000563</td>\n",
       "      <td>1.002288</td>\n",
       "      <td>1.004220</td>\n",
       "      <td>1.006344</td>\n",
       "      <td>1.008645</td>\n",
       "      <td>1.011110</td>\n",
       "      <td>1.013723</td>\n",
       "      <td>1.016471</td>\n",
       "      <td>1.019338</td>\n",
       "      <td>1.022312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9997</th>\n",
       "      <td>10.181228</td>\n",
       "      <td>6.181228</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>0.030587</td>\n",
       "      <td>0.033143</td>\n",
       "      <td>0.035582</td>\n",
       "      <td>0.038020</td>\n",
       "      <td>0.040572</td>\n",
       "      <td>0.043356</td>\n",
       "      <td>0.046485</td>\n",
       "      <td>...</td>\n",
       "      <td>0.992527</td>\n",
       "      <td>0.996200</td>\n",
       "      <td>1.000256</td>\n",
       "      <td>1.004671</td>\n",
       "      <td>1.009423</td>\n",
       "      <td>1.014490</td>\n",
       "      <td>1.019850</td>\n",
       "      <td>1.025479</td>\n",
       "      <td>1.031355</td>\n",
       "      <td>1.037456</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9998</th>\n",
       "      <td>10.156260</td>\n",
       "      <td>6.020380</td>\n",
       "      <td>4.135880</td>\n",
       "      <td>0.030699</td>\n",
       "      <td>0.033247</td>\n",
       "      <td>0.035682</td>\n",
       "      <td>0.038120</td>\n",
       "      <td>0.040676</td>\n",
       "      <td>0.043465</td>\n",
       "      <td>0.046604</td>\n",
       "      <td>...</td>\n",
       "      <td>0.994741</td>\n",
       "      <td>0.997739</td>\n",
       "      <td>1.001036</td>\n",
       "      <td>1.004613</td>\n",
       "      <td>1.008451</td>\n",
       "      <td>1.012532</td>\n",
       "      <td>1.016838</td>\n",
       "      <td>1.021350</td>\n",
       "      <td>1.026048</td>\n",
       "      <td>1.030916</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999</th>\n",
       "      <td>9.684431</td>\n",
       "      <td>5.368862</td>\n",
       "      <td>4.315569</td>\n",
       "      <td>0.030927</td>\n",
       "      <td>0.033503</td>\n",
       "      <td>0.035965</td>\n",
       "      <td>0.038432</td>\n",
       "      <td>0.041022</td>\n",
       "      <td>0.043855</td>\n",
       "      <td>0.047050</td>\n",
       "      <td>...</td>\n",
       "      <td>0.995139</td>\n",
       "      <td>0.996878</td>\n",
       "      <td>0.998857</td>\n",
       "      <td>1.001069</td>\n",
       "      <td>1.003506</td>\n",
       "      <td>1.006159</td>\n",
       "      <td>1.009022</td>\n",
       "      <td>1.012084</td>\n",
       "      <td>1.015339</td>\n",
       "      <td>1.018778</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10000 rows × 103 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             CN    Num Cu    Num Te       Mu1       Mu2       Mu3       Mu4  \\\n",
       "0      9.920706  5.641016  4.279690  0.030303  0.032757  0.035112  0.037475   \n",
       "1      9.038954  4.685766  4.353189  0.031529  0.034154  0.036671  0.039203   \n",
       "2      9.725820  5.599930  4.125890  0.030576  0.033095  0.035505  0.037921   \n",
       "3      9.374672  5.139779  4.234893  0.030891  0.033427  0.035861  0.038308   \n",
       "4      9.599938  5.271755  4.328182  0.031077  0.033681  0.036170  0.038665   \n",
       "...         ...       ...       ...       ...       ...       ...       ...   \n",
       "9995   9.360022  5.084340  4.275681  0.030950  0.033483  0.035915  0.038363   \n",
       "9996   9.758551  5.435938  4.322612  0.030927  0.033519  0.035994  0.038473   \n",
       "9997  10.181228  6.181228  4.000000  0.030587  0.033143  0.035582  0.038020   \n",
       "9998  10.156260  6.020380  4.135880  0.030699  0.033247  0.035682  0.038120   \n",
       "9999   9.684431  5.368862  4.315569  0.030927  0.033503  0.035965  0.038432   \n",
       "\n",
       "           Mu5       Mu6       Mu7  ...      Mu91      Mu92      Mu93  \\\n",
       "0     0.039958  0.042668  0.045714  ...  0.994119  0.995957  0.998090   \n",
       "1     0.041875  0.044810  0.048132  ...  0.999269  1.000942  1.002782   \n",
       "2     0.040456  0.043223  0.046335  ...  0.994823  0.997591  1.000637   \n",
       "3     0.040883  0.043703  0.046883  ...  0.999852  1.002114  1.004576   \n",
       "4     0.041287  0.044156  0.047393  ...  0.998475  1.000035  1.001807   \n",
       "...        ...       ...       ...  ...       ...       ...       ...   \n",
       "9995  0.040943  0.043770  0.046960  ...  1.001789  1.003715  1.005820   \n",
       "9996  0.041074  0.043918  0.047124  ...  1.000563  1.002288  1.004220   \n",
       "9997  0.040572  0.043356  0.046485  ...  0.992527  0.996200  1.000256   \n",
       "9998  0.040676  0.043465  0.046604  ...  0.994741  0.997739  1.001036   \n",
       "9999  0.041022  0.043855  0.047050  ...  0.995139  0.996878  0.998857   \n",
       "\n",
       "          Mu94      Mu95      Mu96      Mu97      Mu98      Mu99     Mu100  \n",
       "0     1.000540  1.003327  1.006472  1.009996  1.013918  1.018260  1.023043  \n",
       "1     1.004773  1.006900  1.009148  1.011502  1.013947  1.016467  1.019047  \n",
       "2     1.003974  1.007616  1.011577  1.015872  1.020514  1.025518  1.030897  \n",
       "3     1.007246  1.010132  1.013241  1.016581  1.020158  1.023981  1.028057  \n",
       "4     1.003776  1.005926  1.008242  1.010709  1.013311  1.016034  1.018861  \n",
       "...        ...       ...       ...       ...       ...       ...       ...  \n",
       "9995  1.008111  1.010590  1.013262  1.016131  1.019201  1.022478  1.025964  \n",
       "9996  1.006344  1.008645  1.011110  1.013723  1.016471  1.019338  1.022312  \n",
       "9997  1.004671  1.009423  1.014490  1.019850  1.025479  1.031355  1.037456  \n",
       "9998  1.004613  1.008451  1.012532  1.016838  1.021350  1.026048  1.030916  \n",
       "9999  1.001069  1.003506  1.006159  1.009022  1.012084  1.015339  1.018778  \n",
       "\n",
       "[10000 rows x 103 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('mu_cn10000.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "use 500 random data to do the Machine Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CN</th>\n",
       "      <th>Num Cu</th>\n",
       "      <th>Num Te</th>\n",
       "      <th>Mu1</th>\n",
       "      <th>Mu2</th>\n",
       "      <th>Mu3</th>\n",
       "      <th>Mu4</th>\n",
       "      <th>Mu5</th>\n",
       "      <th>Mu6</th>\n",
       "      <th>Mu7</th>\n",
       "      <th>...</th>\n",
       "      <th>Mu91</th>\n",
       "      <th>Mu92</th>\n",
       "      <th>Mu93</th>\n",
       "      <th>Mu94</th>\n",
       "      <th>Mu95</th>\n",
       "      <th>Mu96</th>\n",
       "      <th>Mu97</th>\n",
       "      <th>Mu98</th>\n",
       "      <th>Mu99</th>\n",
       "      <th>Mu100</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5812</th>\n",
       "      <td>9.545194</td>\n",
       "      <td>5.331012</td>\n",
       "      <td>4.214182</td>\n",
       "      <td>0.030818</td>\n",
       "      <td>0.033339</td>\n",
       "      <td>0.035758</td>\n",
       "      <td>0.038189</td>\n",
       "      <td>0.040748</td>\n",
       "      <td>0.043549</td>\n",
       "      <td>0.046706</td>\n",
       "      <td>...</td>\n",
       "      <td>0.996015</td>\n",
       "      <td>0.998384</td>\n",
       "      <td>1.001001</td>\n",
       "      <td>1.003868</td>\n",
       "      <td>1.006988</td>\n",
       "      <td>1.010361</td>\n",
       "      <td>1.013991</td>\n",
       "      <td>1.017879</td>\n",
       "      <td>1.022027</td>\n",
       "      <td>1.026437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6777</th>\n",
       "      <td>9.432370</td>\n",
       "      <td>5.046692</td>\n",
       "      <td>4.385678</td>\n",
       "      <td>0.031002</td>\n",
       "      <td>0.033578</td>\n",
       "      <td>0.036044</td>\n",
       "      <td>0.038519</td>\n",
       "      <td>0.041122</td>\n",
       "      <td>0.043971</td>\n",
       "      <td>0.047185</td>\n",
       "      <td>...</td>\n",
       "      <td>1.002972</td>\n",
       "      <td>1.004174</td>\n",
       "      <td>1.005505</td>\n",
       "      <td>1.006973</td>\n",
       "      <td>1.008586</td>\n",
       "      <td>1.010351</td>\n",
       "      <td>1.012276</td>\n",
       "      <td>1.014369</td>\n",
       "      <td>1.016637</td>\n",
       "      <td>1.019087</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7809</th>\n",
       "      <td>9.758658</td>\n",
       "      <td>5.269701</td>\n",
       "      <td>4.488957</td>\n",
       "      <td>0.030317</td>\n",
       "      <td>0.032747</td>\n",
       "      <td>0.035085</td>\n",
       "      <td>0.037438</td>\n",
       "      <td>0.039913</td>\n",
       "      <td>0.042620</td>\n",
       "      <td>0.045665</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000023</td>\n",
       "      <td>1.000434</td>\n",
       "      <td>1.001038</td>\n",
       "      <td>1.001874</td>\n",
       "      <td>1.002977</td>\n",
       "      <td>1.004387</td>\n",
       "      <td>1.006141</td>\n",
       "      <td>1.008275</td>\n",
       "      <td>1.010829</td>\n",
       "      <td>1.013839</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7167</th>\n",
       "      <td>9.562890</td>\n",
       "      <td>4.984243</td>\n",
       "      <td>4.578647</td>\n",
       "      <td>0.030842</td>\n",
       "      <td>0.033379</td>\n",
       "      <td>0.035811</td>\n",
       "      <td>0.038255</td>\n",
       "      <td>0.040827</td>\n",
       "      <td>0.043644</td>\n",
       "      <td>0.046822</td>\n",
       "      <td>...</td>\n",
       "      <td>1.004146</td>\n",
       "      <td>1.003839</td>\n",
       "      <td>1.003624</td>\n",
       "      <td>1.003523</td>\n",
       "      <td>1.003557</td>\n",
       "      <td>1.003745</td>\n",
       "      <td>1.004110</td>\n",
       "      <td>1.004671</td>\n",
       "      <td>1.005448</td>\n",
       "      <td>1.006464</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1721</th>\n",
       "      <td>9.650359</td>\n",
       "      <td>5.650359</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>0.030548</td>\n",
       "      <td>0.033014</td>\n",
       "      <td>0.035385</td>\n",
       "      <td>0.037772</td>\n",
       "      <td>0.040284</td>\n",
       "      <td>0.043031</td>\n",
       "      <td>0.046124</td>\n",
       "      <td>...</td>\n",
       "      <td>0.989225</td>\n",
       "      <td>0.993046</td>\n",
       "      <td>0.997236</td>\n",
       "      <td>1.001799</td>\n",
       "      <td>1.006741</td>\n",
       "      <td>1.012067</td>\n",
       "      <td>1.017782</td>\n",
       "      <td>1.023892</td>\n",
       "      <td>1.030400</td>\n",
       "      <td>1.037313</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3551</th>\n",
       "      <td>9.676530</td>\n",
       "      <td>5.399556</td>\n",
       "      <td>4.276975</td>\n",
       "      <td>0.030850</td>\n",
       "      <td>0.033386</td>\n",
       "      <td>0.035817</td>\n",
       "      <td>0.038262</td>\n",
       "      <td>0.040834</td>\n",
       "      <td>0.043651</td>\n",
       "      <td>0.046829</td>\n",
       "      <td>...</td>\n",
       "      <td>0.994903</td>\n",
       "      <td>0.996726</td>\n",
       "      <td>0.998783</td>\n",
       "      <td>1.001075</td>\n",
       "      <td>1.003602</td>\n",
       "      <td>1.006366</td>\n",
       "      <td>1.009366</td>\n",
       "      <td>1.012603</td>\n",
       "      <td>1.016079</td>\n",
       "      <td>1.019793</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1959</th>\n",
       "      <td>9.418616</td>\n",
       "      <td>5.002855</td>\n",
       "      <td>4.415760</td>\n",
       "      <td>0.030515</td>\n",
       "      <td>0.032969</td>\n",
       "      <td>0.035331</td>\n",
       "      <td>0.037709</td>\n",
       "      <td>0.040213</td>\n",
       "      <td>0.042953</td>\n",
       "      <td>0.046039</td>\n",
       "      <td>...</td>\n",
       "      <td>1.002441</td>\n",
       "      <td>1.003058</td>\n",
       "      <td>1.003823</td>\n",
       "      <td>1.004778</td>\n",
       "      <td>1.005967</td>\n",
       "      <td>1.007430</td>\n",
       "      <td>1.009210</td>\n",
       "      <td>1.011351</td>\n",
       "      <td>1.013893</td>\n",
       "      <td>1.016880</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6064</th>\n",
       "      <td>9.745420</td>\n",
       "      <td>5.379795</td>\n",
       "      <td>4.365626</td>\n",
       "      <td>0.030329</td>\n",
       "      <td>0.032803</td>\n",
       "      <td>0.035173</td>\n",
       "      <td>0.037550</td>\n",
       "      <td>0.040045</td>\n",
       "      <td>0.042771</td>\n",
       "      <td>0.045836</td>\n",
       "      <td>...</td>\n",
       "      <td>0.995902</td>\n",
       "      <td>0.996860</td>\n",
       "      <td>0.998025</td>\n",
       "      <td>0.999444</td>\n",
       "      <td>1.001169</td>\n",
       "      <td>1.003248</td>\n",
       "      <td>1.005731</td>\n",
       "      <td>1.008668</td>\n",
       "      <td>1.012107</td>\n",
       "      <td>1.016100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4541</th>\n",
       "      <td>9.419857</td>\n",
       "      <td>5.015733</td>\n",
       "      <td>4.404124</td>\n",
       "      <td>0.030815</td>\n",
       "      <td>0.033325</td>\n",
       "      <td>0.035737</td>\n",
       "      <td>0.038165</td>\n",
       "      <td>0.040723</td>\n",
       "      <td>0.043525</td>\n",
       "      <td>0.046686</td>\n",
       "      <td>...</td>\n",
       "      <td>1.001062</td>\n",
       "      <td>1.002235</td>\n",
       "      <td>1.003580</td>\n",
       "      <td>1.005113</td>\n",
       "      <td>1.006847</td>\n",
       "      <td>1.008797</td>\n",
       "      <td>1.010979</td>\n",
       "      <td>1.013407</td>\n",
       "      <td>1.016096</td>\n",
       "      <td>1.019060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9592</th>\n",
       "      <td>9.663118</td>\n",
       "      <td>5.663118</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>0.030743</td>\n",
       "      <td>0.033210</td>\n",
       "      <td>0.035588</td>\n",
       "      <td>0.037987</td>\n",
       "      <td>0.040518</td>\n",
       "      <td>0.043292</td>\n",
       "      <td>0.046419</td>\n",
       "      <td>...</td>\n",
       "      <td>0.989453</td>\n",
       "      <td>0.993486</td>\n",
       "      <td>0.997847</td>\n",
       "      <td>1.002531</td>\n",
       "      <td>1.007531</td>\n",
       "      <td>1.012841</td>\n",
       "      <td>1.018455</td>\n",
       "      <td>1.024368</td>\n",
       "      <td>1.030574</td>\n",
       "      <td>1.037066</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>500 rows × 103 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            CN    Num Cu    Num Te       Mu1       Mu2       Mu3       Mu4  \\\n",
       "5812  9.545194  5.331012  4.214182  0.030818  0.033339  0.035758  0.038189   \n",
       "6777  9.432370  5.046692  4.385678  0.031002  0.033578  0.036044  0.038519   \n",
       "7809  9.758658  5.269701  4.488957  0.030317  0.032747  0.035085  0.037438   \n",
       "7167  9.562890  4.984243  4.578647  0.030842  0.033379  0.035811  0.038255   \n",
       "1721  9.650359  5.650359  4.000000  0.030548  0.033014  0.035385  0.037772   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "3551  9.676530  5.399556  4.276975  0.030850  0.033386  0.035817  0.038262   \n",
       "1959  9.418616  5.002855  4.415760  0.030515  0.032969  0.035331  0.037709   \n",
       "6064  9.745420  5.379795  4.365626  0.030329  0.032803  0.035173  0.037550   \n",
       "4541  9.419857  5.015733  4.404124  0.030815  0.033325  0.035737  0.038165   \n",
       "9592  9.663118  5.663118  4.000000  0.030743  0.033210  0.035588  0.037987   \n",
       "\n",
       "           Mu5       Mu6       Mu7  ...      Mu91      Mu92      Mu93  \\\n",
       "5812  0.040748  0.043549  0.046706  ...  0.996015  0.998384  1.001001   \n",
       "6777  0.041122  0.043971  0.047185  ...  1.002972  1.004174  1.005505   \n",
       "7809  0.039913  0.042620  0.045665  ...  1.000023  1.000434  1.001038   \n",
       "7167  0.040827  0.043644  0.046822  ...  1.004146  1.003839  1.003624   \n",
       "1721  0.040284  0.043031  0.046124  ...  0.989225  0.993046  0.997236   \n",
       "...        ...       ...       ...  ...       ...       ...       ...   \n",
       "3551  0.040834  0.043651  0.046829  ...  0.994903  0.996726  0.998783   \n",
       "1959  0.040213  0.042953  0.046039  ...  1.002441  1.003058  1.003823   \n",
       "6064  0.040045  0.042771  0.045836  ...  0.995902  0.996860  0.998025   \n",
       "4541  0.040723  0.043525  0.046686  ...  1.001062  1.002235  1.003580   \n",
       "9592  0.040518  0.043292  0.046419  ...  0.989453  0.993486  0.997847   \n",
       "\n",
       "          Mu94      Mu95      Mu96      Mu97      Mu98      Mu99     Mu100  \n",
       "5812  1.003868  1.006988  1.010361  1.013991  1.017879  1.022027  1.026437  \n",
       "6777  1.006973  1.008586  1.010351  1.012276  1.014369  1.016637  1.019087  \n",
       "7809  1.001874  1.002977  1.004387  1.006141  1.008275  1.010829  1.013839  \n",
       "7167  1.003523  1.003557  1.003745  1.004110  1.004671  1.005448  1.006464  \n",
       "1721  1.001799  1.006741  1.012067  1.017782  1.023892  1.030400  1.037313  \n",
       "...        ...       ...       ...       ...       ...       ...       ...  \n",
       "3551  1.001075  1.003602  1.006366  1.009366  1.012603  1.016079  1.019793  \n",
       "1959  1.004778  1.005967  1.007430  1.009210  1.011351  1.013893  1.016880  \n",
       "6064  0.999444  1.001169  1.003248  1.005731  1.008668  1.012107  1.016100  \n",
       "4541  1.005113  1.006847  1.008797  1.010979  1.013407  1.016096  1.019060  \n",
       "9592  1.002531  1.007531  1.012841  1.018455  1.024368  1.030574  1.037066  \n",
       "\n",
       "[500 rows x 103 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test=df.sample(500)\n",
    "df_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "single output*(predict Num Cu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of our features is: (500, 100)\n",
      "The shape of our labels is: (500,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([5.33101222, 5.04669191, 5.26970141, 4.98424282, 5.65035908,\n",
       "       5.43464435, 5.21883703, 5.20273527, 4.68699058, 4.75289482,\n",
       "       4.98923776, 4.80284787, 4.        , 5.98609163, 5.20577362,\n",
       "       4.06097159, 4.61954074, 5.13609126, 4.14185824, 5.52871418,\n",
       "       6.        , 5.21117299, 4.        , 5.45580741, 4.70345514,\n",
       "       5.58912171, 4.8589529 , 5.96343795, 4.63464197, 4.36540761,\n",
       "       5.21259835, 5.56824947, 4.68522668, 4.85289477, 5.15525588,\n",
       "       4.29166995, 5.73738917, 4.33498478, 5.13515025, 5.58629502,\n",
       "       5.318484  , 4.22585232, 5.73461784, 5.13443397, 4.57834721,\n",
       "       5.41848596, 5.02462231, 4.44097288, 6.38557507, 4.92604033,\n",
       "       5.34577892, 5.02338572, 4.53154572, 5.44875149, 6.19410918,\n",
       "       5.51483973, 4.51530296, 5.24925484, 4.25979368, 4.46596684,\n",
       "       5.54655143, 4.77457736, 4.27448464, 4.959767  , 5.57271175,\n",
       "       5.38170026, 5.1950723 , 4.77536325, 5.17969162, 5.34993514,\n",
       "       4.69122203, 5.18911272, 5.70405928, 5.66362961, 4.93827947,\n",
       "       5.08356511, 4.91706683, 4.83189155, 5.25127564, 5.95557633,\n",
       "       4.95249829, 4.89063497, 5.46671943, 5.46241159, 5.61535535,\n",
       "       4.62471203, 4.98418989, 4.6400986 , 5.47413706, 4.66663141,\n",
       "       5.26384691, 5.69409477, 4.6300675 , 5.50100204, 4.72674259,\n",
       "       6.32416648, 5.77291598, 4.91767478, 5.88821419, 5.76123196,\n",
       "       4.50207985, 4.98864884, 4.43919664, 4.52255313, 5.28855464,\n",
       "       4.72392968, 5.66544342, 5.05568784, 4.87625052, 5.85153405,\n",
       "       5.21705749, 5.60723953, 5.78878605, 6.79031441, 5.76935911,\n",
       "       5.14277951, 4.64536756, 7.26204781, 4.59284193, 5.38517728,\n",
       "       5.61698011, 4.88041276, 5.60980735, 5.5436958 , 4.63143511,\n",
       "       5.35354138, 4.9663716 , 5.22761683, 5.35314578, 4.06022726,\n",
       "       4.93970979, 5.25824718, 5.13071692, 6.04332955, 5.30922221,\n",
       "       5.89668873, 5.86700744, 5.11300077, 6.        , 4.54416686,\n",
       "       4.76325637, 5.1047205 , 4.32062328, 5.13827123, 4.86254249,\n",
       "       4.85250399, 4.61938398, 5.12033188, 4.42200483, 5.17646209,\n",
       "       4.65966041, 5.08716385, 4.        , 4.78860661, 5.29160756,\n",
       "       4.51634584, 5.52868455, 6.86513059, 5.65880064, 6.02215965,\n",
       "       4.73650459, 5.00688115, 5.26008208, 4.07691434, 6.05062763,\n",
       "       5.52664669, 5.69561296, 4.98263721, 5.20790081, 6.46323148,\n",
       "       5.64317836, 5.09933221, 5.58388139, 5.16729267, 6.12442636,\n",
       "       6.15506984, 5.18248881, 4.97496994, 5.43507487, 5.82690207,\n",
       "       5.33666679, 5.60353782, 5.14781341, 5.03368378, 6.32290607,\n",
       "       5.32384332, 5.8999668 , 5.46256453, 5.78024087, 5.20574307,\n",
       "       4.34549988, 5.69028454, 5.5837639 , 5.7545172 , 5.07286779,\n",
       "       4.72529091, 4.56540621, 5.67646079, 4.26425897, 5.02511209,\n",
       "       5.17325928, 5.13324706, 4.7895001 , 4.61627594, 4.63433769,\n",
       "       6.29800887, 5.0996313 , 5.98620533, 5.03805586, 5.32786991,\n",
       "       4.        , 5.66492601, 5.70615313, 4.88473107, 4.80100915,\n",
       "       4.70298161, 5.52513932, 5.71897978, 4.        , 5.0543729 ,\n",
       "       6.3765406 , 5.10226132, 5.28891418, 4.993684  , 5.10176841,\n",
       "       4.49124615, 5.57434042, 4.7808908 , 5.3041688 , 5.11099823,\n",
       "       6.02330519, 6.        , 5.02143002, 5.286508  , 6.68253361,\n",
       "       5.15553963, 5.7137647 , 5.69139118, 4.95749054, 5.1764209 ,\n",
       "       4.85679798, 4.82773416, 4.90674679, 5.74671641, 5.64203512,\n",
       "       4.48860211, 5.37809905, 4.20647522, 5.33450477, 4.20676292,\n",
       "       5.34992929, 4.18838744, 4.34193725, 5.84957365, 4.89734062,\n",
       "       5.9460449 , 4.44378648, 4.82717132, 4.31068086, 5.19080549,\n",
       "       4.        , 5.19693294, 4.50387799, 4.62445162, 4.70062969,\n",
       "       5.08010639, 5.16086682, 5.31728006, 4.62545873, 4.24241018,\n",
       "       5.29034692, 4.931536  , 4.81042922, 4.77927075, 5.15544031,\n",
       "       5.04641383, 4.9099098 , 5.32097533, 4.81028388, 5.81648686,\n",
       "       6.76788126, 5.35882969, 5.45051661, 5.21175972, 5.12960885,\n",
       "       4.94851121, 5.18857191, 4.83737048, 5.23993937, 4.9920184 ,\n",
       "       4.98869349, 5.37285246, 5.49942087, 5.11346469, 4.86600898,\n",
       "       4.53437011, 5.3057294 , 7.26254855, 5.28470217, 5.60422651,\n",
       "       5.58414697, 5.62717469, 5.29215386, 4.89121763, 5.08108306,\n",
       "       5.47557775, 5.34140614, 5.66549197, 5.16458922, 5.31655429,\n",
       "       4.79229392, 5.20406456, 6.80793858, 5.53702882, 5.47118917,\n",
       "       5.47306186, 4.53282349, 4.53007966, 4.71880891, 5.64473722,\n",
       "       5.12885972, 5.8068256 , 4.75945288, 5.11408312, 5.27830545,\n",
       "       5.82092351, 5.55399965, 5.36315587, 5.00233903, 5.8192818 ,\n",
       "       4.99193234, 4.94663295, 5.22662039, 5.12328378, 5.6701203 ,\n",
       "       4.65822184, 5.22462651, 5.25993128, 4.95296965, 5.88276199,\n",
       "       4.52208954, 4.49256042, 4.400329  , 5.15710278, 4.91008865,\n",
       "       4.7257302 , 4.91216854, 4.86313116, 5.70627771, 5.33747715,\n",
       "       6.06065452, 5.42510135, 4.70782107, 6.08800755, 4.7470227 ,\n",
       "       5.0123663 , 6.04986079, 5.30925675, 4.73799952, 5.05249584,\n",
       "       5.62152904, 4.89255574, 4.66588929, 4.95102439, 5.48361257,\n",
       "       5.4080774 , 4.86100951, 4.67608888, 5.86968869, 4.93926045,\n",
       "       4.83738376, 5.97382731, 6.41917045, 6.01206545, 4.30376743,\n",
       "       4.66949526, 4.74668581, 4.70369205, 5.78763423, 4.65150221,\n",
       "       6.01012716, 4.88215875, 4.96719615, 5.07214244, 5.59374105,\n",
       "       5.12234662, 6.94615926, 5.24886779, 5.19649614, 4.93762791,\n",
       "       4.76365995, 4.41643687, 4.62013694, 4.48282894, 6.        ,\n",
       "       5.18229687, 4.88004349, 5.46205502, 5.9366183 , 4.77798866,\n",
       "       5.49474843, 5.01902259, 4.77658865, 5.53789014, 6.        ,\n",
       "       4.84949268, 4.6739106 , 5.70807819, 5.57992181, 5.12326225,\n",
       "       4.40913867, 4.74928107, 4.        , 5.41337211, 5.30797698,\n",
       "       5.47549609, 5.70592217, 5.42349664, 5.97150223, 5.65434134,\n",
       "       5.16337073, 6.87294158, 5.76374685, 4.81422107, 5.06679345,\n",
       "       4.96915494, 5.94759707, 5.10915213, 5.28440354, 5.56191267,\n",
       "       5.52137373, 6.33076318, 5.05970231, 5.731381  , 5.72537743,\n",
       "       5.16827582, 4.78822972, 5.69661743, 5.15361003, 4.78622696,\n",
       "       4.98560736, 4.57738998, 6.06189265, 5.58073741, 5.34542092,\n",
       "       6.23748691, 6.83958357, 4.90813008, 4.9805788 , 5.81891584,\n",
       "       4.82415965, 4.88654862, 5.24071851, 5.41359911, 4.        ,\n",
       "       4.91250813, 6.14297765, 5.03426574, 5.53210642, 5.37339302,\n",
       "       5.19257947, 4.38534553, 4.45239818, 4.77244077, 4.89095393,\n",
       "       5.46267542, 5.05459069, 4.97720368, 4.7675909 , 4.76451052,\n",
       "       5.40287389, 5.68897535, 5.09531073, 4.89724666, 6.13207985,\n",
       "       5.65871079, 5.11029885, 4.53204181, 4.63917193, 4.37416648,\n",
       "       5.01781203, 5.23099731, 5.59421464, 5.17608997, 4.51551575,\n",
       "       4.72951834, 4.69221053, 5.78154404, 4.68490712, 4.60823353,\n",
       "       4.91486504, 4.84352449, 5.08811631, 4.97069019, 4.43476313,\n",
       "       5.39955569, 5.00285547, 5.37979453, 5.01573285, 5.66311845])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "features=np.array(df_test.loc[:,'Mu1':'Mu100'])\n",
    "#y = np.array(df[\"Num Cu\"])\n",
    "labels = np.array(df_test['Num Cu'])\n",
    "print('The shape of our features is:', features.shape)\n",
    "print('The shape of our labels is:', labels.shape)\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using Skicit-learn to split data into training and testing sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "# Split the data into training and testing sets\n",
    "train_features, test_features, train_labels, test_labels = train_test_split(features, labels, test_size = 0.25, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Features Shape: (375, 100)\n",
      "Training Labels Shape: (375,)\n",
      "Testing Features Shape: (125, 100)\n",
      "Testing Labels Shape: (125,)\n"
     ]
    }
   ],
   "source": [
    "print('Training Features Shape:', train_features.shape)\n",
    "print('Training Labels Shape:', train_labels.shape)\n",
    "print('Testing Features Shape:', test_features.shape)\n",
    "print('Testing Labels Shape:', test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the model we are using\n",
    "import sklearn.gaussian_process as gp\n",
    "kernel = gp.kernels.ConstantKernel(1.0, (1e-1, 1e3)) * gp.kernels.RBF(10.0, (1e-3, 1e3))\n",
    "model = gp.GaussianProcessRegressor(kernel=kernel, n_restarts_optimizer=10, alpha=0.1, normalize_y=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/lib/python3.7/site-packages/sklearn/gaussian_process/_gpr.py:494: ConvergenceWarning: lbfgs failed to converge (status=2):\n",
      "ABNORMAL_TERMINATION_IN_LNSRCH.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  _check_optimize_result(\"lbfgs\", opt_res)\n"
     ]
    }
   ],
   "source": [
    "gpr = model.fit(train_features, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.003302761824909699\n"
     ]
    }
   ],
   "source": [
    "params = model.kernel_.get_params()\n",
    "pred_labels, std = model.predict(test_features, return_std=True)\n",
    "MSE = ((pred_labels-test_labels)**2).mean()\n",
    "print(MSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R Squared Error from gpr score: 0.9888327839158607\n"
     ]
    }
   ],
   "source": [
    "print('R Squared Error from gpr score:',gpr.score(train_features, train_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R Squared Error: 0.9890911246840579\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "print('R Squared Error:', r2_score(test_labels, pred_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "predict only Num of Te"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of our features is: (500, 100)\n",
      "The shape of our labels is: (500,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([4.21418186, 4.38567831, 4.48895695, 4.57864714, 4.        ,\n",
       "       4.33220182, 4.55452291, 4.18143267, 4.47774187, 4.4522781 ,\n",
       "       4.53330587, 4.59600457, 5.        , 4.22754405, 4.40229945,\n",
       "       4.96951421, 4.80822712, 4.38460627, 4.77985757, 4.37802501,\n",
       "       4.        , 4.46264565, 5.        , 4.40268359, 4.4907188 ,\n",
       "       4.02232506, 4.32452189, 4.09006513, 4.58103222, 4.61158483,\n",
       "       4.27841546, 4.28225636, 4.27486231, 4.10100261, 4.22017926,\n",
       "       4.58732836, 4.19357947, 4.53278814, 4.13173364, 4.3285605 ,\n",
       "       4.19328738, 4.92471589, 4.42827751, 4.19917997, 4.55182264,\n",
       "       4.37776159, 4.35878767, 4.77003802, 4.        , 4.15465358,\n",
       "       4.2132252 , 4.47817587, 4.65063745, 4.19522892, 4.10525847,\n",
       "       4.34050872, 4.21215321, 4.26775875, 4.46165779, 4.76701658,\n",
       "       4.27831232, 4.51566303, 4.23942299, 4.16533433, 4.29656793,\n",
       "       4.08948258, 4.26835549, 4.48318995, 4.25891305, 4.38541302,\n",
       "       4.60435566, 4.28025591, 4.04471569, 4.4454568 , 4.58176449,\n",
       "       4.34253404, 4.49496801, 4.34658733, 4.39339268, 4.1719434 ,\n",
       "       4.49506861, 4.64437989, 4.30611752, 4.45375389, 4.37912164,\n",
       "       4.68764399, 4.42358314, 4.6799507 , 4.11741952, 4.66668429,\n",
       "       4.42148658, 4.12369116, 4.34580054, 4.249583  , 4.58652591,\n",
       "       4.15333364, 4.16513577, 4.61846358, 4.34587633, 4.17566237,\n",
       "       4.67635915, 4.37332304, 4.46697367, 4.67218209, 4.08358989,\n",
       "       4.16546469, 4.00310056, 4.48567971, 4.38458684, 4.04502017,\n",
       "       4.27521575, 4.19638023, 4.35455711, 4.        , 4.40486744,\n",
       "       4.34336214, 4.55301308, 4.        , 4.61768517, 4.35943648,\n",
       "       4.08434667, 4.26494804, 4.24339304, 4.2281521 , 4.49533725,\n",
       "       4.23987867, 4.69844924, 4.42141228, 4.27283887, 4.93977274,\n",
       "       4.30352268, 4.15346385, 4.37401889, 4.17105957, 4.29236422,\n",
       "       4.29798614, 4.        , 4.05142401, 4.        , 4.71549977,\n",
       "       4.38568666, 4.30066079, 4.77684572, 4.3995123 , 4.35925287,\n",
       "       4.14749601, 4.35370518, 4.29579085, 4.7671861 , 4.44755903,\n",
       "       4.50397958, 4.4604854 , 4.06539413, 4.50253319, 4.49644225,\n",
       "       4.75532319, 4.        , 4.09525941, 4.37398844, 4.        ,\n",
       "       4.43291659, 4.24809846, 4.15314975, 4.06640232, 4.        ,\n",
       "       4.1929453 , 4.21805731, 4.32189041, 4.21784257, 4.        ,\n",
       "       4.10539857, 4.28159238, 4.19848676, 4.47178702, 4.11465203,\n",
       "       4.12024518, 4.17415298, 4.35459784, 4.        , 4.27085407,\n",
       "       4.33224821, 4.33432338, 4.22630098, 4.14599652, 4.        ,\n",
       "       4.09626711, 4.30712709, 4.39653527, 4.18341739, 4.43084399,\n",
       "       4.60117416, 4.09904164, 4.15526946, 4.08021136, 4.55640106,\n",
       "       4.49970648, 4.45369764, 4.00773571, 4.58253442, 4.48744395,\n",
       "       4.36354514, 4.50431924, 4.21395442, 4.5957551 , 4.19272552,\n",
       "       4.01817226, 4.48585946, 4.16468299, 4.34266804, 4.31438037,\n",
       "       4.15245786, 4.08397484, 4.34548874, 4.58298515, 4.47868985,\n",
       "       4.6485092 , 4.12966079, 4.07698872, 5.        , 4.37554395,\n",
       "       4.16283823, 4.21620656, 4.01893427, 4.38791011, 4.37135377,\n",
       "       4.80564953, 4.3615141 , 4.51040859, 4.28982807, 4.12119012,\n",
       "       4.2509575 , 4.        , 4.57928641, 4.23577299, 4.00080972,\n",
       "       4.35093689, 4.28512945, 4.10185363, 4.49322281, 4.17809507,\n",
       "       4.40367503, 4.70786103, 4.5114031 , 4.19691714, 4.33934444,\n",
       "       4.75569894, 4.25889861, 4.4314014 , 4.3147057 , 4.61724671,\n",
       "       4.29983624, 4.93720419, 4.69455512, 4.08672108, 4.34505124,\n",
       "       4.07207969, 4.6095235 , 4.4519701 , 4.75221805, 4.42178237,\n",
       "       5.        , 4.37756014, 4.49612201, 4.68852008, 4.50262091,\n",
       "       4.48200158, 4.        , 4.24002711, 4.72269326, 4.45006463,\n",
       "       4.56988436, 4.46415812, 4.67008172, 4.55742624, 4.48799597,\n",
       "       4.4127823 , 4.3256117 , 4.32363528, 4.63520782, 4.1454013 ,\n",
       "       4.        , 4.22596746, 4.30324584, 4.        , 4.51189469,\n",
       "       4.31755326, 4.22600057, 4.66926017, 4.41462426, 4.28535311,\n",
       "       4.47812075, 4.40848787, 4.33641201, 4.47091696, 4.13399102,\n",
       "       4.55198491, 4.34696365, 4.15903258, 4.32837393, 4.16350251,\n",
       "       4.33308854, 4.21231561, 4.19947199, 4.55386518, 4.53508846,\n",
       "       4.26657681, 4.23153367, 4.00062778, 4.34884604, 4.34172285,\n",
       "       4.21253315, 4.56490516, 4.        , 4.1335991 , 4.37938302,\n",
       "       4.25472386, 4.75699909, 4.6105558 , 4.64059554, 4.22805572,\n",
       "       4.19498925, 4.08480137, 4.66153879, 4.29803155, 4.29796407,\n",
       "       4.08953825, 4.22809994, 4.1990306 , 4.66959143, 4.01595896,\n",
       "       4.24868709, 4.51103912, 4.49288288, 4.34718592, 4.2592982 ,\n",
       "       4.        , 4.31320528, 4.26175474, 4.21241217, 4.13681453,\n",
       "       4.55442451, 4.75919592, 4.74108083, 4.42813877, 4.37003485,\n",
       "       4.66606568, 4.52982329, 4.43146709, 4.32995903, 4.21333968,\n",
       "       4.07529793, 4.3181163 , 4.57282762, 4.08540973, 4.53337279,\n",
       "       4.        , 4.23206585, 4.04383906, 4.44536995, 4.47375208,\n",
       "       4.19240146, 4.36496767, 4.67947986, 4.49673882, 4.5271414 ,\n",
       "       4.47797361, 4.43516099, 4.55692526, 4.12382849, 4.60696843,\n",
       "       4.51567781, 4.01308635, 4.11184491, 4.00248457, 4.89874419,\n",
       "       4.66525237, 4.63579843, 4.58417384, 4.2378513 , 4.63171652,\n",
       "       4.        , 4.4240136 , 4.03280385, 4.5168566 , 4.0469244 ,\n",
       "       4.2191865 , 4.05181117, 4.00443576, 4.27540081, 4.5153592 ,\n",
       "       4.40990269, 4.58356313, 4.47846576, 4.39645211, 4.        ,\n",
       "       4.37496614, 4.32252029, 4.24278642, 4.13793029, 4.37903046,\n",
       "       4.24186316, 4.39339353, 4.39374221, 4.25535235, 4.        ,\n",
       "       4.59086941, 4.3260894 , 4.27451617, 4.13460394, 4.36874833,\n",
       "       4.36693771, 4.44220877, 5.        , 4.25986924, 4.3823919 ,\n",
       "       4.17240032, 4.21369956, 4.51398174, 4.10805565, 4.15110795,\n",
       "       4.4411272 , 4.        , 4.27373058, 4.69356816, 4.42027234,\n",
       "       4.45052439, 4.34517358, 4.57768554, 4.26288197, 4.30246164,\n",
       "       4.12126812, 4.15601893, 4.49744891, 4.31847914, 4.29987628,\n",
       "       4.58743464, 4.37175874, 4.45333157, 4.42754871, 4.55107015,\n",
       "       4.38137882, 4.55872371, 4.12019273, 4.16210508, 4.39148523,\n",
       "       4.02401619, 4.05347214, 4.14168502, 4.46290808, 4.26771692,\n",
       "       4.57060203, 4.63403834, 4.31237417, 4.377175  , 5.        ,\n",
       "       4.48265647, 4.        , 4.30738954, 4.11270255, 4.27916719,\n",
       "       4.37019181, 4.        , 4.79991017, 4.22755923, 4.5217709 ,\n",
       "       4.29573395, 4.31030019, 4.55779973, 4.1520954 , 4.62985741,\n",
       "       4.23190192, 4.37845865, 4.19030762, 4.5230358 , 4.17190032,\n",
       "       4.03843493, 4.34059843, 4.17753036, 4.62098985, 4.65803226,\n",
       "       4.5179225 , 4.2932634 , 4.15871255, 4.03813459, 4.59875271,\n",
       "       4.60323392, 4.58979752, 4.1967966 , 4.43721483, 4.44260354,\n",
       "       4.36505904, 4.15647551, 4.11719777, 4.17647799, 4.65392322,\n",
       "       4.2769747 , 4.41576025, 4.3656256 , 4.40412447, 4.        ])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features=np.array(df_test.loc[:,'Mu1':'Mu100'])\n",
    "#y = np.array(df[\"Num Cu\"])\n",
    "labels = np.array(df_test['Num Te'])\n",
    "print('The shape of our features is:', features.shape)\n",
    "print('The shape of our labels is:', labels.shape)\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Features Shape: (375, 100)\n",
      "Training Labels Shape: (375,)\n",
      "Testing Features Shape: (125, 100)\n",
      "Testing Labels Shape: (125,)\n"
     ]
    }
   ],
   "source": [
    "train_features, test_features, train_labels, test_labels = train_test_split(features, labels, test_size = 0.25, random_state = 42)\n",
    "print('Training Features Shape:', train_features.shape)\n",
    "print('Training Labels Shape:', train_labels.shape)\n",
    "print('Testing Features Shape:', test_features.shape)\n",
    "print('Testing Labels Shape:', test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpr = model.fit(train_features, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.00045932475141681304\n"
     ]
    }
   ],
   "source": [
    "params = model.kernel_.get_params()\n",
    "pred_labels, std = model.predict(test_features, return_std=True)\n",
    "MSE = ((pred_labels-test_labels)**2).mean()\n",
    "print(MSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R Squared Error: 0.9909071362859572\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "print('R Squared Error:', r2_score(test_labels, pred_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## From above, for individual prediction, by using spectra, the num of Te get higher R2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# use spectra predict Num of Cu, Te as multi target regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of our features is: (500, 100)\n",
      "The shape of our labels is: (500, 2)\n",
      "Training Features Shape: (375, 100)\n",
      "Training Labels Shape: (375, 2)\n",
      "Testing Features Shape: (125, 100)\n",
      "Testing Labels Shape: (125, 2)\n"
     ]
    }
   ],
   "source": [
    "# Multi target regression\n",
    "features=np.array(df_test.loc[:,'Mu1':'Mu100'])\n",
    "labels = np.array(df_test.loc[:,'Num Cu':'Num Te'])\n",
    "print('The shape of our features is:', features.shape)\n",
    "print('The shape of our labels is:', labels.shape)\n",
    "train_features, test_features, train_labels, test_labels = train_test_split(features, labels, test_size = 0.25, random_state = 42)\n",
    "print('Training Features Shape:', train_features.shape)\n",
    "print('Training Labels Shape:', train_labels.shape)\n",
    "print('Testing Features Shape:', test_features.shape)\n",
    "print('Testing Labels Shape:', test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpr = model.fit(train_features, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0025314003580414798\n"
     ]
    }
   ],
   "source": [
    "params = model.kernel_.get_params()\n",
    "pred_labels, std = model.predict(test_features, return_std=True)\n",
    "MSE = ((pred_labels-test_labels)**2).mean()\n",
    "print(MSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R Squared Error from gpr score for MTR: 0.9861623213520035\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/lib/python3.7/site-packages/sklearn/base.py:434: FutureWarning: The default value of multioutput (not exposed in score method) will change from 'variance_weighted' to 'uniform_average' in 0.23 to keep consistent with 'metrics.r2_score'. To specify the default value manually and avoid the warning, please either call 'metrics.r2_score' directly or make a custom scorer with 'metrics.make_scorer' (the built-in scorer 'r2' uses multioutput='uniform_average').\n",
      "  \"multioutput='uniform_average').\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "print('R Squared Error from gpr score for MTR:',gpr.score(train_features, train_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R Squared Error: 0.99064863475574\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "print('R Squared Error:', r2_score(test_labels, pred_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[4.99209526, 4.3599297 ],\n",
       "       [5.74697924, 4.4353725 ],\n",
       "       [4.34148324, 4.89444868],\n",
       "       [4.51705017, 4.7566998 ],\n",
       "       [5.2747812 , 4.08617602],\n",
       "       [6.0917904 , 4.00067571],\n",
       "       [4.70428703, 4.58643183],\n",
       "       [4.68368721, 4.5015001 ],\n",
       "       [5.17721283, 4.25239223],\n",
       "       [4.82204718, 4.56120256],\n",
       "       [4.64239531, 4.46042485],\n",
       "       [4.99892278, 4.55503368],\n",
       "       [4.74766754, 4.33574021],\n",
       "       [5.60659629, 4.39271488],\n",
       "       [5.99106829, 4.01757627],\n",
       "       [5.15829085, 4.27834558],\n",
       "       [5.45499822, 4.28689254],\n",
       "       [5.22393409, 4.27673441],\n",
       "       [4.65745038, 4.74596272],\n",
       "       [5.64270494, 4.14126818],\n",
       "       [4.92836772, 4.37835314],\n",
       "       [5.09377586, 4.19452001],\n",
       "       [6.5310612 , 4.01516578],\n",
       "       [5.98599604, 4.21542172],\n",
       "       [4.99190257, 4.49469661],\n",
       "       [4.33627586, 4.03536354],\n",
       "       [5.3669451 , 4.35644757],\n",
       "       [5.754282  , 4.07850417],\n",
       "       [5.00027251, 4.37968337],\n",
       "       [5.63430745, 4.26992943],\n",
       "       [5.61715636, 4.0305877 ],\n",
       "       [5.26772517, 4.32785836],\n",
       "       [4.99165592, 4.37314873],\n",
       "       [5.16026875, 4.4733496 ],\n",
       "       [5.23665554, 4.47064023],\n",
       "       [5.10199795, 4.3569732 ],\n",
       "       [5.13751624, 4.36733947],\n",
       "       [4.63993233, 4.60696842],\n",
       "       [5.32203847, 4.31822645],\n",
       "       [4.93463175, 4.16678159],\n",
       "       [5.59597195, 4.07139056],\n",
       "       [5.56238335, 4.25181468],\n",
       "       [4.83166673, 4.59310697],\n",
       "       [5.351351  , 4.08261684],\n",
       "       [4.90390221, 4.1291313 ],\n",
       "       [4.81023179, 4.35187125],\n",
       "       [5.31568638, 4.21320824],\n",
       "       [4.84193615, 4.5986227 ],\n",
       "       [5.42492978, 4.17227224],\n",
       "       [3.98820151, 4.99157228],\n",
       "       [5.69158324, 4.04868463],\n",
       "       [5.20167903, 4.22464035],\n",
       "       [5.29841631, 4.16999258],\n",
       "       [4.34614512, 4.38681951],\n",
       "       [5.70862102, 4.07848275],\n",
       "       [5.55037905, 4.33722406],\n",
       "       [4.42524388, 4.7700138 ],\n",
       "       [4.23369602, 4.75754175],\n",
       "       [4.63294888, 4.20954254],\n",
       "       [5.2213489 , 4.38773213],\n",
       "       [4.48337275, 4.65797966],\n",
       "       [4.40816159, 4.47944516],\n",
       "       [5.11063225, 4.30303035],\n",
       "       [5.21591645, 4.03104413],\n",
       "       [5.94288366, 4.16441106],\n",
       "       [5.58433109, 4.31792484],\n",
       "       [4.84967803, 4.63252672],\n",
       "       [5.00475672, 4.37412601],\n",
       "       [4.95800718, 4.48875072],\n",
       "       [4.61265611, 4.562086  ],\n",
       "       [4.81928296, 4.58313978],\n",
       "       [4.47598134, 4.76256365],\n",
       "       [5.28991182, 4.31875683],\n",
       "       [4.92668838, 4.47309416],\n",
       "       [4.535896  , 4.6129782 ],\n",
       "       [4.97851358, 4.53662359],\n",
       "       [4.56658218, 4.72802843],\n",
       "       [5.27354304, 4.39583777],\n",
       "       [4.90936281, 4.48826941],\n",
       "       [5.48614406, 4.28963729],\n",
       "       [5.25092516, 4.48553637],\n",
       "       [5.35602003, 4.3172605 ],\n",
       "       [4.54564635, 4.18996204],\n",
       "       [5.58604817, 4.1934253 ],\n",
       "       [5.6572883 , 4.08483863],\n",
       "       [5.334218  , 4.40468762],\n",
       "       [5.36877974, 4.32672952],\n",
       "       [5.32307729, 4.37876305],\n",
       "       [6.68446298, 4.02009032],\n",
       "       [4.7960711 , 4.63898215],\n",
       "       [4.85833015, 4.42367738],\n",
       "       [5.07634916, 4.2006215 ],\n",
       "       [4.63636594, 4.67210236],\n",
       "       [5.34516002, 4.28211426],\n",
       "       [6.33175285, 4.14276591],\n",
       "       [4.86677497, 4.4317804 ],\n",
       "       [5.00510079, 4.43907368],\n",
       "       [5.14582149, 4.34432587],\n",
       "       [5.15146179, 4.42166308],\n",
       "       [4.03421363, 4.97279335],\n",
       "       [4.22472958, 4.62781034],\n",
       "       [5.74747954, 4.31553413],\n",
       "       [5.51763846, 4.38918547],\n",
       "       [4.78267491, 4.66731411],\n",
       "       [5.12296105, 4.49835397],\n",
       "       [4.48321793, 4.23554182],\n",
       "       [5.62819226, 4.21297521],\n",
       "       [5.10378512, 4.14358176],\n",
       "       [4.8364755 , 4.51205651],\n",
       "       [5.13562195, 4.37443052],\n",
       "       [5.05145075, 4.03337369],\n",
       "       [4.83589062, 4.69493032],\n",
       "       [4.63720751, 4.00973161],\n",
       "       [5.6966624 , 4.02817643],\n",
       "       [4.88482043, 4.51323059],\n",
       "       [5.38770771, 4.22175627],\n",
       "       [4.30023427, 4.44127629],\n",
       "       [5.69535643, 4.10048766],\n",
       "       [7.04077996, 4.01518421],\n",
       "       [5.59102149, 4.43536796],\n",
       "       [6.22783513, 4.17282694],\n",
       "       [5.2549527 , 4.18479578],\n",
       "       [5.16146844, 4.20110529],\n",
       "       [4.77106097, 4.49994109],\n",
       "       [6.19255851, 3.99301562]])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from the above, we can see that if we use multi target regression, the predicted values are still reasonable.\n",
    "In the following, I am trying to use 1st and/or 2nd derivate to predict the number of atoms by multi target regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "generate a derivate dataframe from the test dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../PredictXANES')\n",
    "import xanes_derivatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "dmu1, dmu2 = xanes_derivatives.xanes_derivatives(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of our features is: (500, 99)\n",
      "The shape of our labels is: (500, 2)\n",
      "Training Features Shape: (375, 99)\n",
      "Training Labels Shape: (375, 2)\n",
      "Testing Features Shape: (125, 99)\n",
      "Testing Labels Shape: (125, 2)\n"
     ]
    }
   ],
   "source": [
    "# use first derivate only \n",
    "features=dmu1\n",
    "labels = np.array(df_test.loc[:,'Num Cu':'Num Te'])\n",
    "print('The shape of our features is:', features.shape)\n",
    "print('The shape of our labels is:', labels.shape)\n",
    "train_features, test_features, train_labels, test_labels = train_test_split(features, labels, test_size = 0.25, random_state = 42)\n",
    "print('Training Features Shape:', train_features.shape)\n",
    "print('Training Labels Shape:', train_labels.shape)\n",
    "print('Testing Features Shape:', test_features.shape)\n",
    "print('Testing Labels Shape:', test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpr = model.fit(train_features, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0023952488907380363\n",
      "R Squared Error: 0.9913572071168851\n"
     ]
    }
   ],
   "source": [
    "params = model.kernel_.get_params()\n",
    "pred_labels, std = model.predict(test_features, return_std=True)\n",
    "MSE = ((pred_labels-test_labels)**2).mean()\n",
    "print(MSE)\n",
    "print('R Squared Error:', r2_score(test_labels, pred_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of our features is: (500, 98)\n",
      "The shape of our labels is: (500, 2)\n",
      "Training Features Shape: (375, 98)\n",
      "Training Labels Shape: (375, 2)\n",
      "Testing Features Shape: (125, 98)\n",
      "Testing Labels Shape: (125, 2)\n"
     ]
    }
   ],
   "source": [
    "# use second derivate only \n",
    "features=dmu2\n",
    "labels = np.array(df_test.loc[:,'Num Cu':'Num Te'])\n",
    "print('The shape of our features is:', features.shape)\n",
    "print('The shape of our labels is:', labels.shape)\n",
    "train_features, test_features, train_labels, test_labels = train_test_split(features, labels, test_size = 0.25, random_state = 42)\n",
    "print('Training Features Shape:', train_features.shape)\n",
    "print('Training Labels Shape:', train_labels.shape)\n",
    "print('Testing Features Shape:', test_features.shape)\n",
    "print('Testing Labels Shape:', test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpr = model.fit(train_features, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0034970737429013995\n",
      "R Squared Error: 0.9869676720711851\n"
     ]
    }
   ],
   "source": [
    "params = model.kernel_.get_params()\n",
    "pred_labels, std = model.predict(test_features, return_std=True)\n",
    "MSE = ((pred_labels-test_labels)**2).mean()\n",
    "print(MSE)\n",
    "print('R Squared Error:', r2_score(test_labels, pred_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# use first and second derivate together, multi target regressor implementing GPR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of first derivate is: (500, 99)\n",
      "The shape of second derivate is: (500, 98)\n",
      "The shape of the concatenate derivate is: (500, 197)\n"
     ]
    }
   ],
   "source": [
    "dmu_both = np.concatenate((dmu1,dmu2),axis=1)\n",
    "print('The shape of first derivate is:', dmu1.shape)\n",
    "print('The shape of second derivate is:', dmu2.shape)\n",
    "print('The shape of the concatenate derivate is:', dmu_both.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of our features is: (500, 197)\n",
      "The shape of our labels is: (500, 2)\n",
      "Training Features Shape: (375, 197)\n",
      "Training Labels Shape: (375, 2)\n",
      "Testing Features Shape: (125, 197)\n",
      "Testing Labels Shape: (125, 2)\n",
      "0.0025206702700705267\n",
      "R Squared Error: 0.9908417036106936\n"
     ]
    }
   ],
   "source": [
    "# use first and second derivate together\n",
    "features=dmu_both\n",
    "labels = np.array(df_test.loc[:,'Num Cu':'Num Te'])\n",
    "print('The shape of our features is:', features.shape)\n",
    "print('The shape of our labels is:', labels.shape)\n",
    "train_features, test_features, train_labels, test_labels = train_test_split(features, labels, test_size = 0.25, random_state = 42)\n",
    "print('Training Features Shape:', train_features.shape)\n",
    "print('Training Labels Shape:', train_labels.shape)\n",
    "print('Testing Features Shape:', test_features.shape)\n",
    "print('Testing Labels Shape:', test_labels.shape)\n",
    "gpr = model.fit(train_features, train_labels)\n",
    "params = model.kernel_.get_params()\n",
    "pred_labels, std = model.predict(test_features, return_std=True)\n",
    "MSE = ((pred_labels-test_labels)**2).mean()\n",
    "print(MSE)\n",
    "print('R Squared Error:', r2_score(test_labels, pred_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# use first and/or derivative to predict num of Cu, Te seperatly "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of our features is: (500, 99)\n",
      "The shape of our labels is: (500,)\n",
      "Training Features Shape: (375, 99)\n",
      "Training Labels Shape: (375,)\n",
      "Testing Features Shape: (125, 99)\n",
      "Testing Labels Shape: (125,)\n",
      "0.002918098379497163\n",
      "R Squared Error: 0.9903616509245388\n"
     ]
    }
   ],
   "source": [
    "# use first derivate to predict Cu \n",
    "features=dmu1\n",
    "labels = np.array(df_test['Num Cu'])\n",
    "print('The shape of our features is:', features.shape)\n",
    "print('The shape of our labels is:', labels.shape)\n",
    "train_features, test_features, train_labels, test_labels = train_test_split(features, labels, test_size = 0.25, random_state = 42)\n",
    "print('Training Features Shape:', train_features.shape)\n",
    "print('Training Labels Shape:', train_labels.shape)\n",
    "print('Testing Features Shape:', test_features.shape)\n",
    "print('Testing Labels Shape:', test_labels.shape)\n",
    "gpr = model.fit(train_features, train_labels)\n",
    "params = model.kernel_.get_params()\n",
    "pred_labels, std = model.predict(test_features, return_std=True)\n",
    "MSE = ((pred_labels-test_labels)**2).mean()\n",
    "print(MSE)\n",
    "print('R Squared Error:', r2_score(test_labels, pred_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### From above, we can see that the R2 is less than using multi targert regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of our features is: (500, 99)\n",
      "The shape of our labels is: (500,)\n",
      "Training Features Shape: (375, 99)\n",
      "Training Labels Shape: (375,)\n",
      "Testing Features Shape: (125, 99)\n",
      "Testing Labels Shape: (125,)\n",
      "0.0006548257469826092\n",
      "R Squared Error: 0.9870369683858905\n"
     ]
    }
   ],
   "source": [
    "# use first derivate to predict Te\n",
    "features=dmu1\n",
    "labels = np.array(df_test['Num Te'])\n",
    "print('The shape of our features is:', features.shape)\n",
    "print('The shape of our labels is:', labels.shape)\n",
    "train_features, test_features, train_labels, test_labels = train_test_split(features, labels, test_size = 0.25, random_state = 42)\n",
    "print('Training Features Shape:', train_features.shape)\n",
    "print('Training Labels Shape:', train_labels.shape)\n",
    "print('Testing Features Shape:', test_features.shape)\n",
    "print('Testing Labels Shape:', test_labels.shape)\n",
    "gpr = model.fit(train_features, train_labels)\n",
    "params = model.kernel_.get_params()\n",
    "pred_labels, std = model.predict(test_features, return_std=True)\n",
    "MSE = ((pred_labels-test_labels)**2).mean()\n",
    "print(MSE)\n",
    "print('R Squared Error:', r2_score(test_labels, pred_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare with spectra, which Te has higher R2, dmu1 will give Cu higher R2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The follwing is using second derivate to predict Cu,Te seperately"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of our features is: (500, 98)\n",
      "The shape of our labels is: (500,)\n",
      "Training Features Shape: (375, 98)\n",
      "Training Labels Shape: (375,)\n",
      "Testing Features Shape: (125, 98)\n",
      "Testing Labels Shape: (125,)\n",
      "0.0042488105215117405\n",
      "R Squared Error: 0.985966367943743\n"
     ]
    }
   ],
   "source": [
    "# use second derivate to predict Cu\n",
    "features=dmu2\n",
    "labels = np.array(df_test['Num Cu'])\n",
    "print('The shape of our features is:', features.shape)\n",
    "print('The shape of our labels is:', labels.shape)\n",
    "train_features, test_features, train_labels, test_labels = train_test_split(features, labels, test_size = 0.25, random_state = 42)\n",
    "print('Training Features Shape:', train_features.shape)\n",
    "print('Training Labels Shape:', train_labels.shape)\n",
    "print('Testing Features Shape:', test_features.shape)\n",
    "print('Testing Labels Shape:', test_labels.shape)\n",
    "gpr = model.fit(train_features, train_labels)\n",
    "params = model.kernel_.get_params()\n",
    "pred_labels, std = model.predict(test_features, return_std=True)\n",
    "MSE = ((pred_labels-test_labels)**2).mean()\n",
    "print(MSE)\n",
    "print('R Squared Error:', r2_score(test_labels, pred_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of our features is: (500, 98)\n",
      "The shape of our labels is: (500,)\n",
      "Training Features Shape: (375, 98)\n",
      "Training Labels Shape: (375,)\n",
      "Testing Features Shape: (125, 98)\n",
      "Testing Labels Shape: (125,)\n",
      "0.000848697416434547\n",
      "R Squared Error: 0.9831990548771957\n"
     ]
    }
   ],
   "source": [
    "# use second derivate to predict Te\n",
    "features=dmu2\n",
    "labels = np.array(df_test['Num Te'])\n",
    "print('The shape of our features is:', features.shape)\n",
    "print('The shape of our labels is:', labels.shape)\n",
    "train_features, test_features, train_labels, test_labels = train_test_split(features, labels, test_size = 0.25, random_state = 42)\n",
    "print('Training Features Shape:', train_features.shape)\n",
    "print('Training Labels Shape:', train_labels.shape)\n",
    "print('Testing Features Shape:', test_features.shape)\n",
    "print('Testing Labels Shape:', test_labels.shape)\n",
    "gpr = model.fit(train_features, train_labels)\n",
    "params = model.kernel_.get_params()\n",
    "pred_labels, std = model.predict(test_features, return_std=True)\n",
    "MSE = ((pred_labels-test_labels)**2).mean()\n",
    "print(MSE)\n",
    "print('R Squared Error:', r2_score(test_labels, pred_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compared with spectra and first derivate, using second derivative data, both R2 for  Cu, Te are lower. Still the Cu has higher R2 compared with Te"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
